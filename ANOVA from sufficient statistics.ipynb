{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, an experiment was run comparing the performance of 5 different AI agents across 60 trials. However, due to an oversight most of the data was lost - save for the means and variances of each sample. Based on this limited data, I attempted to carry out a one-way ANOVA through manual calculation. There are at least two conceivable ways to run the ANOVA. The first, listed in code below, is to reconstruct a surrogate data-set retaining the summary statistics of the original data for an ANOVA. The second is to carry out the ANOVA through the grand mean, for which the napkin maths is listed at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, csv\n",
    "import pandas as pd \n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_mean_table = [75.17, 99.5, 96.75, 91.08, 72.58]\n",
    "ss_variance_table = [6884.72, 5015.85, 3610.87, 5373.81, 2992.79]\n",
    "surrogate_data = []\n",
    "n = 60\n",
    "k = 5\n",
    "N = n * k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to reconstruct some surrogate data-set based on the limited statistics available. The mean and variances of our original data are sufficient statistics for an ANOVA. This is accomplised through the following:\n",
    "\n",
    "$$y_i = \\overline{x} + \\frac{s}{\\sqrt{n}}, i = 1, 2, ...., n - 1$$\n",
    "\n",
    "and \n",
    "\n",
    "$$y_n = n\\overline{x} - (n - 1)y_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for mean, variance in zip(ss_mean_table, ss_variance_table):\n",
    "    count += 1\n",
    "    reconstructed_sample = []\n",
    "    for i in range(0, 58):\n",
    "        y_i = mean + math.sqrt(variance) / math.sqrt(n)\n",
    "        reconstructed_sample.append([y_i, count])\n",
    "    y_n = n*mean - (n - 1)*reconstructed_sample[0][0]\n",
    "    reconstructed_sample.append([y_n, count])\n",
    "    surrogate_data.append(reconstructed_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now re-organise this data into a dataframe and begin our analysis in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          score  sample\n",
      "0     85.881925       1\n",
      "1     85.881925       1\n",
      "2     85.881925       1\n",
      "3     85.881925       1\n",
      "4     85.881925       1\n",
      "5     85.881925       1\n",
      "6     85.881925       1\n",
      "7     85.881925       1\n",
      "8     85.881925       1\n",
      "9     85.881925       1\n",
      "10    85.881925       1\n",
      "11    85.881925       1\n",
      "12    85.881925       1\n",
      "13    85.881925       1\n",
      "14    85.881925       1\n",
      "15    85.881925       1\n",
      "16    85.881925       1\n",
      "17    85.881925       1\n",
      "18    85.881925       1\n",
      "19    85.881925       1\n",
      "20    85.881925       1\n",
      "21    85.881925       1\n",
      "22    85.881925       1\n",
      "23    85.881925       1\n",
      "24    85.881925       1\n",
      "25    85.881925       1\n",
      "26    85.881925       1\n",
      "27    85.881925       1\n",
      "28    85.881925       1\n",
      "29    85.881925       1\n",
      "..          ...     ...\n",
      "265   79.642566       5\n",
      "266   79.642566       5\n",
      "267   79.642566       5\n",
      "268   79.642566       5\n",
      "269   79.642566       5\n",
      "270   79.642566       5\n",
      "271   79.642566       5\n",
      "272   79.642566       5\n",
      "273   79.642566       5\n",
      "274   79.642566       5\n",
      "275   79.642566       5\n",
      "276   79.642566       5\n",
      "277   79.642566       5\n",
      "278   79.642566       5\n",
      "279   79.642566       5\n",
      "280   79.642566       5\n",
      "281   79.642566       5\n",
      "282   79.642566       5\n",
      "283   79.642566       5\n",
      "284   79.642566       5\n",
      "285   79.642566       5\n",
      "286   79.642566       5\n",
      "287   79.642566       5\n",
      "288   79.642566       5\n",
      "289   79.642566       5\n",
      "290   79.642566       5\n",
      "291   79.642566       5\n",
      "292   79.642566       5\n",
      "293   79.642566       5\n",
      "294 -344.111372       5\n",
      "\n",
      "[295 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('output.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in surrogate_data:\n",
    "        writer.writerows(i)\n",
    "data = pd.read_csv('output.csv', header=None)\n",
    "data.columns=['score', 'sample']\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction factor = 2188901.320023551\n"
     ]
    }
   ],
   "source": [
    "sum_n = data['score'].sum()\n",
    "sum_n_sq = (sum_n)**2\n",
    "corection_factor = sum_n_sq / N\n",
    "print('correction factor = ' + str(corection_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss total = 1481836.0060398462\n"
     ]
    }
   ],
   "source": [
    "sum_list = []\n",
    "for i in data['score']:\n",
    "    sum_list.append(i**2)\n",
    "ss_total = np.sum(sum_list) - corection_factor\n",
    "print('ss total = ' + str(ss_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss group = 35730.730605458375\n"
     ]
    }
   ],
   "source": [
    "groupvar_toadd = []\n",
    "summed_groupby = data.groupby(['sample']).sum()\n",
    "for index, row in summed_groupby.iterrows():\n",
    "    grouped_sq = row.values[0]**2\n",
    "    grouped_div = grouped_sq/n\n",
    "    groupvar_toadd.append(grouped_div)\n",
    "ss_group = np.sum(groupvar_toadd) - corection_factor\n",
    "print('ss group = ' + str(ss_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss error = 1446105.2754343878\n"
     ]
    }
   ],
   "source": [
    "ss_error = ss_total - ss_group\n",
    "print('ss error = ' + str(ss_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms group = 8932.682651364594\n"
     ]
    }
   ],
   "source": [
    "ms_group = ss_group / (k - 1)\n",
    "print('ms group = ' + str(ms_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms error = 4902.0517811335185\n"
     ]
    }
   ],
   "source": [
    "ms_error = ss_error / (N - k)\n",
    "print('ms error = ' + str(ms_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance ratio = 1.8222334341191024\n"
     ]
    }
   ],
   "source": [
    "variance_ratio = ms_group / ms_error\n",
    "print('variance ratio = ' + str(variance_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variance ratio is our Fisher statistic. I am pretty sure that it is in fact incorrect. At bare minimum it is slightly divergent from the original F statistic, due to the imperfect reconstruction of the data. At worst, there is an error in my manual calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
